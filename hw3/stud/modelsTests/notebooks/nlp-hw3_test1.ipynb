{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe2040e",
   "metadata": {
    "papermill": {
     "duration": 0.078963,
     "end_time": "2022-03-31T22:14:09.386664",
     "exception": false,
     "start_time": "2022-03-31T22:14:09.307701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/nlp2022-hw3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424a631",
   "metadata": {
    "papermill": {
     "duration": 0.049244,
     "end_time": "2022-03-31T22:14:12.552634",
     "exception": false,
     "start_time": "2022-03-31T22:14:12.503390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Important paths for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_root = '../../../../'\n",
    "test_name = 'test1'\n",
    "datasets_path = os.path.join(app_root, 'data')\n",
    "model_dir_path = os.path.join(app_root, 'model', test_name)\n",
    "\n",
    "data_train_path = os.path.join(datasets_path, 'train.tsv')\n",
    "data_dev_path = os.path.join(datasets_path, 'dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('../../../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc95ee",
   "metadata": {
    "papermill": {
     "duration": 0.048851,
     "end_time": "2022-03-31T22:14:13.015246",
     "exception": false,
     "start_time": "2022-03-31T22:14:12.966395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting the seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 28\n",
    "\n",
    "# random.seed(SEED) # not used\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'batch_size': 32,\n",
    "    'PAD_TOKEN': '<pad>',\n",
    "    'UNK_TOKEN': '<unk>',\n",
    "    'transformer_name': \"bert-base-cased\",\n",
    "    'mention_tags': {\n",
    "        'p_open':  '<P>', 'p_close':  '</P>', \n",
    "        'e_open':  '<E>', 'e_close':  '</E>', \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9214a0",
   "metadata": {
    "papermill": {
     "duration": 0.049393,
     "end_time": "2022-03-31T22:14:13.226777",
     "exception": false,
     "start_time": "2022-03-31T22:14:13.177384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stud.modelsTests.dataset.Dataset_transformer_simple import Dataset_transformer_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset_transformer_simple(\n",
    "    data_train_path, \n",
    "    tokenizer = global_params['transformer_name'],\n",
    "    split_by_entities = True)\n",
    "\n",
    "dataset_dev = Dataset_transformer_simple(\n",
    "    data_dev_path, \n",
    "    tokenizer = global_params['transformer_name'],\n",
    "    split_by_entities = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train-15',\n",
       " 'text': \"Twenty years ago, Lorenzo Uribe discovered true love with Maria Herrera and began a romance. Lorenzo was rich, married, and had a young son: Lautaro. Maria was poor and unknown to Lorenzo, had a daughter called Renata. Maria's mother, Gracia, wanted her daughter to catch this rich man at all costs and convinced her that pregnancy would assure this.\",\n",
       " 'pron': 'her',\n",
       " 'p_offset': 250,\n",
       " 'entity_A': 'Maria',\n",
       " 'offset_A': 219,\n",
       " 'is_coref_A': 'TRUE',\n",
       " 'entity_B': 'Gracia',\n",
       " 'offset_B': 235,\n",
       " 'is_coref_B': 'FALSE'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.data_raw[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "15\n",
      "17\n",
      "28\n",
      "31\n",
      "33\n",
      "45\n",
      "46\n",
      "49\n",
      "55\n",
      "67\n",
      "78\n",
      "80\n",
      "88\n",
      "92\n",
      "97\n",
      "99\n",
      "101\n",
      "112\n",
      "114\n",
      "121\n",
      "122\n",
      "141\n",
      "155\n",
      "178\n",
      "180\n",
      "190\n",
      "200\n",
      "223\n",
      "233\n",
      "239\n",
      "244\n",
      "254\n",
      "256\n",
      "279\n",
      "334\n",
      "335\n",
      "340\n",
      "348\n",
      "349\n",
      "350\n",
      "357\n",
      "368\n",
      "374\n",
      "376\n",
      "394\n",
      "399\n",
      "404\n",
      "406\n",
      "412\n",
      "417\n",
      "426\n",
      "428\n",
      "438\n",
      "452\n",
      "486\n",
      "500\n",
      "504\n",
      "513\n",
      "528\n",
      "529\n",
      "531\n",
      "534\n",
      "540\n",
      "557\n",
      "564\n",
      "581\n",
      "632\n",
      "656\n",
      "661\n",
      "668\n",
      "675\n",
      "687\n",
      "720\n",
      "732\n",
      "733\n",
      "745\n",
      "784\n",
      "815\n",
      "833\n",
      "834\n",
      "838\n",
      "847\n",
      "872\n",
      "873\n",
      "882\n",
      "885\n",
      "900\n",
      "905\n",
      "915\n",
      "917\n",
      "923\n",
      "926\n",
      "936\n",
      "941\n",
      "968\n",
      "982\n",
      "984\n",
      "988\n",
      "996\n",
      "998\n",
      "1005\n",
      "1006\n",
      "1012\n",
      "1043\n",
      "1046\n",
      "1088\n",
      "1103\n",
      "1109\n",
      "1130\n",
      "1146\n",
      "1151\n",
      "1160\n",
      "1163\n",
      "1170\n",
      "1173\n",
      "1180\n",
      "1182\n",
      "1183\n",
      "1194\n",
      "1230\n",
      "1235\n",
      "1237\n",
      "1239\n",
      "1243\n",
      "1248\n",
      "1251\n",
      "1252\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1264\n",
      "1270\n",
      "1272\n",
      "1282\n",
      "1289\n",
      "1290\n",
      "1298\n",
      "1304\n",
      "1307\n",
      "1331\n",
      "1374\n",
      "1390\n",
      "1396\n",
      "1408\n",
      "1426\n",
      "1461\n",
      "1470\n",
      "1499\n",
      "1506\n",
      "1531\n",
      "1546\n",
      "1559\n",
      "1566\n",
      "1576\n",
      "1581\n",
      "1582\n",
      "1607\n",
      "1618\n",
      "1619\n",
      "1625\n",
      "1629\n",
      "1631\n",
      "1633\n",
      "1638\n",
      "1641\n",
      "1643\n",
      "1682\n",
      "1694\n",
      "1702\n",
      "1703\n",
      "1729\n",
      "1744\n",
      "1746\n",
      "1750\n",
      "1752\n",
      "1753\n",
      "1755\n",
      "1776\n",
      "1810\n",
      "1818\n",
      "1828\n",
      "1836\n",
      "1840\n",
      "1856\n",
      "1862\n",
      "1869\n",
      "1911\n",
      "1919\n",
      "1921\n",
      "1925\n",
      "1931\n",
      "1937\n",
      "1948\n",
      "1949\n",
      "1951\n",
      "1961\n",
      "1983\n",
      "1985\n",
      "1986\n",
      "1990\n",
      "2011\n",
      "2028\n",
      "2041\n",
      "2049\n",
      "2071\n",
      "2082\n",
      "2099\n",
      "2106\n",
      "2116\n",
      "2120\n",
      "2122\n",
      "2123\n",
      "2128\n",
      "2134\n",
      "2154\n",
      "2156\n",
      "2182\n",
      "2186\n",
      "2187\n",
      "2190\n",
      "2192\n",
      "2196\n",
      "2226\n",
      "2271\n",
      "2297\n",
      "2309\n",
      "2313\n",
      "2325\n",
      "2336\n",
      "2350\n",
      "2365\n",
      "2377\n",
      "2389\n",
      "2395\n",
      "2405\n",
      "2422\n",
      "2451\n",
      "2461\n",
      "2463\n",
      "2470\n",
      "2485\n",
      "2506\n",
      "2509\n",
      "2511\n",
      "2512\n",
      "2521\n",
      "2524\n",
      "2530\n",
      "2534\n",
      "2540\n",
      "2543\n",
      "2546\n",
      "2556\n",
      "2570\n",
      "2590\n",
      "2598\n",
      "2605\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2614\n",
      "2615\n",
      "2635\n",
      "2643\n",
      "2649\n",
      "2650\n",
      "2659\n",
      "2668\n",
      "2675\n",
      "2676\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2688\n",
      "2704\n",
      "2720\n",
      "2727\n",
      "2729\n",
      "2730\n",
      "2733\n",
      "2746\n",
      "2749\n",
      "2760\n",
      "2763\n",
      "2764\n",
      "2779\n",
      "2784\n",
      "2794\n",
      "2807\n",
      "2809\n",
      "2826\n",
      "2829\n",
      "2836\n",
      "2843\n",
      "2849\n",
      "2864\n",
      "2878\n",
      "2884\n",
      "2885\n",
      "2890\n",
      "2900\n",
      "2901\n",
      "2913\n",
      "2931\n",
      "2937\n",
      "2944\n",
      "2950\n",
      "2960\n",
      "2967\n",
      "2970\n",
      "2971\n",
      "2980\n",
      "2988\n",
      "2996\n"
     ]
    }
   ],
   "source": [
    "for i,e in enumerate(dataset_train.data_raw):\n",
    "    if e['is_coref_A'] == e['is_coref_B'] and e['is_coref_A'] == 'FALSE':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('his', 273), (None, None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'train-7',\n",
       " 'text': \"Reb Chaim Yaakov's wife is the sister of Rabbi Moishe Sternbuch, as is the wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his uncles. Reb Asher's brother Rabbi Shlomo Arieli is the author of a critical edition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he later studied under his father-in-law in the Mirrer Yeshiva.\",\n",
       " 'pron': 'his',\n",
       " 'p_offset': 273,\n",
       " 'entity_A': 'Reb Asher',\n",
       " 'offset_A': 152,\n",
       " 'is_coref_A': 'FALSE',\n",
       " 'entity_B': 'Akiva Eiger',\n",
       " 'offset_B': 253,\n",
       " 'is_coref_B': 'FALSE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing both entities as FALSE (pass (None, None)):\n",
    "\n",
    "i_test = 6\n",
    "\n",
    "ex_sample = (\n",
    "    (dataset_train.data_raw[i_test]['pron'], dataset_train.data_raw[i_test]['p_offset']),\n",
    "    (None, None),\n",
    ")\n",
    "ex_label = dataset_train.data_raw[i_test]\n",
    "print(ex_sample)\n",
    "ex_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'TRUE': 2684, 'FALSE': 3314})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "for s in dataset_train:\n",
    "    for e in s['entities']:\n",
    "        c[e['is_coref']] += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params.update({ \n",
    "    'token_embeddings_len': len(dataset_train.tokenizer),\n",
    "    'resize_token_embeddings': False, # ! not used\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(model_dir_path, 'global_params.npy'), global_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=global_params['batch_size'],\n",
    "    collate_fn=dataset_train.create_collate_fn(),\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataloader_dev = DataLoader(\n",
    "    dataset_dev,\n",
    "    batch_size=global_params['batch_size'],\n",
    "    collate_fn=dataset_dev.create_collate_fn(),\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in dataloader_dev:\n",
    "    ex_in = e\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'words_ids', 'binary_is_coref'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_in.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  7085,  9724,  1358,   146,  7831,  1161,   113,  1983,   117,\n",
      "        10978,  2349,   114,   117, 14680,  6385,  1186, 19569, 21571,  9552,\n",
      "          113,  1983,   117, 10978,  2349,   146,   118,  2684,   114,   117,\n",
      "        10785, 23294,  2155, 16140,  9800,  2047,   113,  1983,   117, 10978,\n",
      "         2349,   146,   118,  2684,  1957,  2596,   114,   117,  1847,  2091,\n",
      "        18824,   113,  1483,   117, 10978,  2349,   114,  1105,  3270,  4409,\n",
      "          113,  1483,   117, 10978,  2349,   146,   118,  2684,   114,  6114,\n",
      "         6750,  2723,   113,  1145,  2752,  1106,  1112,  1615,  6750,  2723,\n",
      "          114,  1110,  1141,  1104,  1103,  2746,  4245,   112,   188,  2020,\n",
      "         4525,  2300, 12392,   119,  1124,  1110,  3025,  1510,   117,  1133,\n",
      "         1674,  1136,  1294,  1251,  2357,  3178,  1235,  1103,  2985,  1544,\n",
      "         1104,  1103,  1326,   119,   102,  1124,   117,  3270,  4409,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0]) torch.Size([32, 241])\n",
      "[-1, 0, 0, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 8, 9, 9, 9, 10, 11, 12, 13, 13, 14, 15, 16, 17, 18, 19, 19, 19, 19, 20, 20, 21, 22, 23, 24, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 43, 44, 45, 45, 46, 47, 48, 49, 50, 51, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, -1, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "print(ex_in['input_ids'][0], ex_in['input_ids'].shape)\n",
    "print(ex_in['words_ids'][0])\n",
    "print(ex_in['binary_is_coref'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stud.modelsTests.utils.print_infos import print_summary, display_history, plot_confusion_matrix, print_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from stud.modelsTests.model_3.model3_transformer_simple import Model3\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "final_model = Model3(\n",
    "    device = device,\n",
    "    loss_fn = loss_function,\n",
    "    model_load_weights = False,\n",
    "    fine_tune_transformer = False,\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(final_model.model.parameters(), lr=0.0016, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model3_net(\n",
      "  (transformer_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (relu): ReLU()\n",
      "  (classifier): Linear(in_features=384, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (loss_fn): BCELoss()\n",
      ")\n",
      "----------------------\n",
      "parameters: 108,606,721\n",
      "trainable parameters: 296,449\n",
      "non-trainable parameters: 108,310,272\n"
     ]
    }
   ],
   "source": [
    "print_summary(final_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# position = 522\n",
    "# text = \"As Emperor Taizong of Tang wanted to enter into an alliance with Xueyantuo against Eastern Tujue, he sent the general Qiao Shiwang (***) as an envoy to Yi'nan, recognizing him the Zhenzhupiqie Khan (or Zhenzhu Khan in short), and awarding him with drums and banners. Yi'nan was very pleased, and he offered tribute to Emperor Taizong. It was said that by this point, <P>his</P> territory stretched from the Mohe to the east, Western Tujue to the west, and Gobi Desert to the south, and that many tribes, including Huige, Bayegu, Adie (**), Tongluo (**), Pugu (**), and Xi (*), all submitted to him.\"\n",
    "# word = 'Adie (**'\n",
    "# regex_subs = r\"\" + re.escape('<X>') + word + re.escape('</X>')\n",
    "# re.sub(re.escape(word), regex_subs, text[position:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[0.3642],\n",
      "        [0.3649]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(0.7320, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "ex_in_simple = dataset_train.create_collate_fn()([dataset_train[0], dataset_train[1]])\n",
    "predictions = final_model.model(\n",
    "    input_ids = ex_in_simple['input_ids'].to(device), \n",
    "    attention_mask = ex_in_simple['attention_mask'].to(device),\n",
    "    token_type_ids = ex_in_simple['token_type_ids'].to(device),\n",
    ")\n",
    "labels = ex_in_simple['binary_is_coref'].to(device)\n",
    "\n",
    "print(labels)\n",
    "print(predictions)\n",
    "print(final_model.model.compute_loss(predictions, labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('her', 274), (None, None))\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(final_model.predict(dataset_train.data_raw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 => avg_loss: 0.701327\n",
      "# Validation loss => 0.687788 | accuracy: 0.136564 #\n",
      "Epoch   1 => avg_loss: 0.692595\n",
      "# Validation loss => 0.694436 | accuracy: 0.462555 #\n",
      "Epoch   2 => avg_loss: 0.696323\n",
      "# Validation loss => 0.682222 | accuracy: 0.136564 #\n",
      "Epoch   3 => avg_loss: 0.689592\n",
      "# Validation loss => 0.681736 | accuracy: 0.143172 #\n",
      "Epoch   4 => avg_loss: 0.695119\n",
      "# Validation loss => 0.689663 | accuracy: 0.136564 #\n",
      "Epoch   5 => avg_loss: 0.690969\n",
      "# Validation loss => 0.695474 | accuracy: 0.436123 #\n",
      "Epoch   6 => avg_loss: 0.688410\n",
      "# Validation loss => 0.691392 | accuracy: 0.411894 #\n",
      "Epoch   7 => avg_loss: 0.690299\n",
      "# Validation loss => 0.683197 | accuracy: 0.138767 #\n",
      "Epoch   8 => avg_loss: 0.691127\n",
      "# Validation loss => 0.680749 | accuracy: 0.136564 #\n",
      "Epoch   9 => avg_loss: 0.690062\n",
      "# Validation loss => 0.681750 | accuracy: 0.136564 #\n",
      "Epoch  10 => avg_loss: 0.690424\n",
      "# Validation loss => 0.683250 | accuracy: 0.136564 #\n",
      "Epoch  11 => avg_loss: 0.688831\n",
      "# Validation loss => 0.679579 | accuracy: 0.151982 #\n",
      "Epoch  12 => avg_loss: 0.690984\n",
      "# Validation loss => 0.689101 | accuracy: 0.136564 #\n",
      "Epoch  13 => avg_loss: 0.691511\n",
      "# Validation loss => 0.680170 | accuracy: 0.136564 #\n",
      "Epoch  14 => avg_loss: 0.688601\n",
      "# Validation loss => 0.680980 | accuracy: 0.149780 #\n",
      "Epoch  15 => avg_loss: 0.687320\n",
      "# Validation loss => 0.686893 | accuracy: 0.136564 #\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodelsTests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mTrainer_model3_transformer_simple\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer_model3_transformer_simple\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer_model3_transformer_simple()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=5'>6</a>\u001b[0m     final_model, optimizer, dataloader_train, dataloader_dev,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=6'>7</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=7'>8</a>\u001b[0m     save_best\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=8'>9</a>\u001b[0m     min_score\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=9'>10</a>\u001b[0m     save_path_name\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_dir_path, \u001b[39m'\u001b[39;49m\u001b[39mmodel3_weights_transformer_simple.pth\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=10'>11</a>\u001b[0m     saved_history\u001b[39m=\u001b[39;49mhistory\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/nlp-hw3_test1.ipynb#ch0000036vscode-remote?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/Marco/Desktop/Magistrale/NLP/projects/2022_hw3/nlp2022-hw3/hw3/stud/modelsTests/notebooks/../../../stud/modelsTests/utils/Trainer.py:54\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, final_model, optimizer, train_dataloader, valid_dataloader, epochs, verbose, save_best, save_path_name, min_score, saved_history, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m step, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     53\u001b[0m     dict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_forward(final_model\u001b[39m.\u001b[39mmodel, sample, device, optimizer \u001b[39m=\u001b[39m optimizer) \u001b[39m# override\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     losses\u001b[39m.\u001b[39mappend(dict_out[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     56\u001b[0m mean_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(losses)\n\u001b[1;32m     57\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mtrain_history\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(mean_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stud.modelsTests.utils.Trainer_model3_transformer_simple import Trainer_model3_transformer_simple\n",
    "\n",
    "trainer = Trainer_model3_transformer_simple()\n",
    "\n",
    "history = trainer.train(\n",
    "    final_model, optimizer, dataloader_train, dataloader_dev,\n",
    "    epochs=60, device=device,\n",
    "    save_best=True, \n",
    "    min_score=0.7,\n",
    "    save_path_name=os.path.join(model_dir_path, 'model3_weights_transformer_simple.pth'),\n",
    "    saved_history=history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f736fa57697717c80caf738108553872322bbbec02a6cb9049e8f17a4d9a2aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
